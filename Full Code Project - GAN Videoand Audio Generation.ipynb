{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45fd6a5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tensorflow\n",
    "!pip install keras\n",
    "!pip install mido\n",
    "!pip install pydub\n",
    "!pip install resampy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bb680d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1\n",
    "\n",
    "import os\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior()\n",
    "import numpy as np\n",
    "\n",
    "import helper\n",
    "from glob import glob\n",
    "import pickle as pkl\n",
    "import scipy.misc\n",
    "from PIL import Image\n",
    "\n",
    "import time\n",
    "\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "do_preprocess = True\n",
    "from_checkpoint = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe97437a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1df3a778",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 3\n",
    "data_dir = '/content/drive/MyDrive/photos2' # Data\n",
    "\n",
    "#Step 4\n",
    "data_resized_dir = \"/content/drive/MyDrive/resized_data\"# Resized data\n",
    "\n",
    "if do_preprocess == True:\n",
    "    if not os.path.isdir(data_resized_dir):\n",
    "        os.mkdir(data_resized_dir)\n",
    "\n",
    "    for each in os.listdir(data_dir):\n",
    "        try:\n",
    "            image = cv2.imread(os.path.join(data_dir, each))\n",
    "            image = cv2.resize(image, (128, 128))\n",
    "            cv2.imwrite(os.path.join(data_resized_dir, each), image)\n",
    "        except Exception as e:\n",
    "            print(str(e))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1881688b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Taken from Udacity Face generator Project\n",
    "import math\n",
    "import os\n",
    "import hashlib\n",
    "from urllib.request import urlretrieve\n",
    "import zipfile\n",
    "import gzip\n",
    "import shutil\n",
    "\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "\n",
    "def _read32(bytestream):\n",
    "    \"\"\"\n",
    "    Read 32-bit integer from bytesteam\n",
    "    :param bytestream: A bytestream\n",
    "    :return: 32-bit integer\n",
    "    \"\"\"\n",
    "    dt = np.dtype(np.uint32).newbyteorder('>')\n",
    "    return np.frombuffer(bytestream.read(4), dtype=dt)[0]\n",
    "\n",
    "\n",
    "def _unzip(save_path, _, database_name, data_path):\n",
    "    \"\"\"\n",
    "    Unzip wrapper with the same interface as _ungzip\n",
    "    :param save_path: The path of the gzip files\n",
    "    :param database_name: Name of database\n",
    "    :param data_path: Path to extract to\n",
    "    :param _: HACK - Used to have to same interface as _ungzip\n",
    "    \"\"\"\n",
    "    print('Extracting {}...'.format(database_name))\n",
    "    with zipfile.ZipFile(save_path) as zf:\n",
    "        zf.extractall(data_path)\n",
    "\n",
    "\n",
    "def _ungzip(save_path, extract_path, database_name, _):\n",
    "    \"\"\"\n",
    "    Unzip a gzip file and extract it to extract_path\n",
    "    :param save_path: The path of the gzip files\n",
    "    :param extract_path: The location to extract the data to\n",
    "    :param database_name: Name of database\n",
    "    :param _: HACK - Used to have to same interface as _unzip\n",
    "    \"\"\"\n",
    "    # Get data from save_path\n",
    "    with open(save_path, 'rb') as f:\n",
    "        with gzip.GzipFile(fileobj=f) as bytestream:\n",
    "            magic = _read32(bytestream)\n",
    "            if magic != 2051:\n",
    "                raise ValueError('Invalid magic number {} in file: {}'.format(magic, f.name))\n",
    "            num_images = _read32(bytestream)\n",
    "            rows = _read32(bytestream)\n",
    "            cols = _read32(bytestream)\n",
    "            buf = bytestream.read(rows * cols * num_images)\n",
    "            data = np.frombuffer(buf, dtype=np.uint8)\n",
    "            data = data.reshape(num_images, rows, cols)\n",
    "\n",
    "    # Save data to extract_path\n",
    "    for image_i, image in enumerate(\n",
    "            tqdm(data, unit='File', unit_scale=True, miniters=1, desc='Extracting {}'.format(database_name))):\n",
    "        Image.fromarray(image, 'L').save(os.path.join(extract_path, 'image_{}.jpg'.format(image_i)))\n",
    "\n",
    "\n",
    "def get_image(image_path, width, height, mode):\n",
    "    \"\"\"\n",
    "    Read image from image_path\n",
    "    :param image_path: Path of image\n",
    "    :param width: Width of image\n",
    "    :param height: Height of image\n",
    "    :param mode: Mode of image\n",
    "    :return: Image data\n",
    "    \"\"\"\n",
    "    image = Image.open(image_path)\n",
    "\n",
    "\n",
    "    return np.array(image.convert(mode))\n",
    "\n",
    "\n",
    "def get_batch(image_files, width, height, mode):\n",
    "    data_batch = np.array(\n",
    "        [get_image(sample_file, width, height, mode) for sample_file in image_files]).astype(np.float32)\n",
    "\n",
    "    # Make sure the images are in 4 dimensions\n",
    "    if len(data_batch.shape) < 4:\n",
    "        data_batch = data_batch.reshape(data_batch.shape + (1,))\n",
    "\n",
    "    return data_batch\n",
    "\n",
    "\n",
    "def images_square_grid(images, mode):\n",
    "    \"\"\"\n",
    "    Save images as a square grid\n",
    "    :param images: Images to be used for the grid\n",
    "    :param mode: The mode to use for images\n",
    "    :return: Image of images in a square grid\n",
    "    \"\"\"\n",
    "    # Get maximum size for square grid of images\n",
    "    save_size = math.floor(np.sqrt(images.shape[0]))\n",
    "\n",
    "    # Scale to 0-255\n",
    "    images = (((images - images.min()) * 255) / (images.max() - images.min())).astype(np.uint8)\n",
    "\n",
    "    # Put images in a square arrangement\n",
    "    images_in_square = np.reshape(\n",
    "            images[:save_size*save_size],\n",
    "            (save_size, save_size, images.shape[1], images.shape[2], images.shape[3]))\n",
    "    if mode == 'L':\n",
    "        images_in_square = np.squeeze(images_in_square, 4)\n",
    "\n",
    "    # Combine images to grid image\n",
    "    new_im = Image.new(mode, (images.shape[1] * save_size, images.shape[2] * save_size))\n",
    "    for col_i, col_images in enumerate(images_in_square):\n",
    "        for image_i, image in enumerate(col_images):\n",
    "            im = Image.fromarray(image, mode)\n",
    "            new_im.paste(im, (col_i * images.shape[1], image_i * images.shape[2]))\n",
    "\n",
    "    return new_im\n",
    "\n",
    "\n",
    "def download_extract(database_name, data_path):\n",
    "    \"\"\"\n",
    "    Download and extract database\n",
    "    :param database_name: Database name\n",
    "    \"\"\"\n",
    "    DATASET_CELEBA_NAME = 'celeba'\n",
    "    DATASET_MNIST_NAME = 'mnist'\n",
    "\n",
    "    if database_name == DATASET_CELEBA_NAME:\n",
    "        url = 'https://s3-us-west-1.amazonaws.com/udacity-dlnfd/datasets/celeba.zip'\n",
    "        hash_code = '00d2c5bc6d35e252742224ab0c1e8fcb'\n",
    "        extract_path = os.path.join(data_path, 'img_align_celeba')\n",
    "        save_path = os.path.join(data_path, 'celeba.zip')\n",
    "        extract_fn = _unzip\n",
    "    elif database_name == DATASET_MNIST_NAME:\n",
    "        url = 'http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz'\n",
    "        hash_code = 'f68b3c2dcbeaaa9fbdd348bbdeb94873'\n",
    "        extract_path = os.path.join(data_path, 'mnist')\n",
    "        save_path = os.path.join(data_path, 'train-images-idx3-ubyte.gz')\n",
    "        extract_fn = _ungzip\n",
    "\n",
    "    if os.path.exists(extract_path):\n",
    "        print('Found {} Data'.format(database_name))\n",
    "        return\n",
    "\n",
    "    if not os.path.exists(data_path):\n",
    "        os.makedirs(data_path)\n",
    "\n",
    "    if not os.path.exists(save_path):\n",
    "        with DLProgress(unit='B', unit_scale=True, miniters=1, desc='Downloading {}'.format(database_name)) as pbar:\n",
    "            urlretrieve(\n",
    "                url,\n",
    "                save_path,\n",
    "                pbar.hook)\n",
    "\n",
    "    assert hashlib.md5(open(save_path, 'rb').read()).hexdigest() == hash_code, \\\n",
    "        '{} file is corrupted.  Remove the file and try again.'.format(save_path)\n",
    "\n",
    "    os.makedirs(extract_path)\n",
    "    try:\n",
    "        extract_fn(save_path, extract_path, database_name, data_path)\n",
    "    except Exception as err:\n",
    "        shutil.rmtree(extract_path)  # Remove extraction folder if there is an error\n",
    "        raise err\n",
    "\n",
    "    # Remove compressed data\n",
    "    os.remove(save_path)\n",
    "\n",
    "\n",
    "class Dataset(object):\n",
    "    \"\"\"\n",
    "    Dataset\n",
    "    \"\"\"\n",
    "    def __init__(self, data_files):\n",
    "        \"\"\"\n",
    "        Initalize the class\n",
    "        :param dataset_name: Database name\n",
    "        :param data_files: List of files in the database\n",
    "        \"\"\"\n",
    "        IMAGE_WIDTH = 128\n",
    "        IMAGE_HEIGHT = 128\n",
    "\n",
    "        self.image_mode = 'RGB'\n",
    "        image_channels = 3\n",
    "\n",
    "        self.data_files = data_files\n",
    "        self.shape = len(data_files), IMAGE_WIDTH, IMAGE_HEIGHT, image_channels\n",
    "\n",
    "    def get_batches(self, batch_size):\n",
    "        \"\"\"\n",
    "        Generate batches\n",
    "        :param batch_size: Batch Size\n",
    "        :return: Batches of data\n",
    "        \"\"\"\n",
    "        IMAGE_MAX_VALUE = 255\n",
    "\n",
    "        current_index = 0\n",
    "        while current_index + batch_size <= self.shape[0]:\n",
    "            data_batch = get_batch(\n",
    "                self.data_files[current_index:current_index + batch_size],\n",
    "                *self.shape[1:3],\n",
    "                self.image_mode)\n",
    "\n",
    "            current_index += batch_size\n",
    "\n",
    "            yield data_batch / IMAGE_MAX_VALUE - 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6d2a2e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "resized_data_filenames = [data_resized_dir+'/'+i for i in os.listdir(data_resized_dir)]\n",
    "show_n_images = 9\n",
    "train_images = get_batch(resized_data_filenames[:show_n_images], 64, 64, 'RGB')\n",
    "plt.imshow(images_square_grid(train_images, 'RGB'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2695362d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 7\n",
    "def model_inputs(real_dim, z_dim):\n",
    "    \"\"\"t\n",
    "    Create he model inputs\n",
    "    :param real_dim: tuple containing width, height and channels\n",
    "    :param z_dim: The dimension of Z\n",
    "    :return: Tuple of (tensor of real input images, tensor of z data, learning rate G, learning rate D)\n",
    "    \"\"\"\n",
    "    inputs_real = tf.placeholder(tf.float32, (None, *real_dim), name='inputs_real')\n",
    "    inputs_z = tf.placeholder(tf.float32, (None, z_dim), name=\"input_z\")\n",
    "    learning_rate_G = tf.placeholder(tf.float32, name=\"learning_rate_G\")\n",
    "    learning_rate_D = tf.placeholder(tf.float32, name=\"learning_rate_D\")\n",
    "\n",
    "    return inputs_real, inputs_z, learning_rate_G, learning_rate_D\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a828c3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 8\n",
    "def generator(z, output_channel_dim, is_train=True):\n",
    "    ''' Build the generator network.\n",
    "\n",
    "        Arguments\n",
    "        ---------\n",
    "        z : Input tensor for the generator\n",
    "        output_channel_dim : Shape of the generator output\n",
    "        n_units : Number of units in hidden layer\n",
    "        reuse : Reuse the variables with tf.variable_scope\n",
    "        alpha : leak parameter for leaky ReLU\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        out:\n",
    "    '''\n",
    "    with tf.variable_scope(\"generator\", reuse= not is_train):\n",
    "\n",
    "        # First FC layer --> 8x8x1024\n",
    "        fc1 = tf.layers.dense(z, 8*8*1024)\n",
    "\n",
    "        # Reshape it\n",
    "        fc1 = tf.reshape(fc1, (-1, 8, 8, 1024))\n",
    "\n",
    "        # Leaky ReLU\n",
    "        fc1 = tf.nn.leaky_relu(fc1, alpha=alpha)\n",
    "\n",
    "\n",
    "        # Transposed conv 1 --> BatchNorm --> LeakyReLU\n",
    "        # 8x8x1024 --> 16x16x512\n",
    "        trans_conv1 = tf.layers.conv2d_transpose(inputs = fc1,\n",
    "                                  filters = 512,\n",
    "                                  kernel_size = [5,5],\n",
    "                                  strides = [2,2],\n",
    "                                  padding = \"SAME\",\n",
    "                                kernel_initializer=tf.truncated_normal_initializer(stddev=0.02),\n",
    "                                name=\"trans_conv1\")\n",
    "\n",
    "        batch_trans_conv1 = tf.layers.batch_normalization(inputs = trans_conv1, training=is_train, epsilon=1e-5, name=\"batch_trans_conv1\")\n",
    "\n",
    "        trans_conv1_out = tf.nn.leaky_relu(batch_trans_conv1, alpha=alpha, name=\"trans_conv1_out\")\n",
    "\n",
    "\n",
    "        # Transposed conv 2 --> BatchNorm --> LeakyReLU\n",
    "        # 16x16x512 --> 32x32x256\n",
    "        trans_conv2 = tf.layers.conv2d_transpose(inputs = trans_conv1_out,\n",
    "                                  filters = 256,\n",
    "                                  kernel_size = [5,5],\n",
    "                                  strides = [2,2],\n",
    "                                  padding = \"SAME\",\n",
    "                                kernel_initializer=tf.truncated_normal_initializer(stddev=0.02),\n",
    "                                name=\"trans_conv2\")\n",
    "\n",
    "        batch_trans_conv2 = tf.layers.batch_normalization(inputs = trans_conv2, training=is_train, epsilon=1e-5, name=\"batch_trans_conv2\")\n",
    "\n",
    "        trans_conv2_out = tf.nn.leaky_relu(batch_trans_conv2, alpha=alpha, name=\"trans_conv2_out\")\n",
    "\n",
    "\n",
    "        # Transposed conv 3 --> BatchNorm --> LeakyReLU\n",
    "        # 32x32x256 --> 64x64x128\n",
    "        trans_conv3 = tf.layers.conv2d_transpose(inputs = trans_conv2_out,\n",
    "                                  filters = 128,\n",
    "                                  kernel_size = [5,5],\n",
    "                                  strides = [2,2],\n",
    "                                  padding = \"SAME\",\n",
    "                                kernel_initializer=tf.truncated_normal_initializer(stddev=0.02),\n",
    "                                name=\"trans_conv3\")\n",
    "\n",
    "        batch_trans_conv3 = tf.layers.batch_normalization(inputs = trans_conv3, training=is_train, epsilon=1e-5, name=\"batch_trans_conv3\")\n",
    "\n",
    "        trans_conv3_out = tf.nn.leaky_relu(batch_trans_conv3, alpha=alpha, name=\"trans_conv3_out\")\n",
    "\n",
    "\n",
    "        # Transposed conv 4 --> BatchNorm --> LeakyReLU\n",
    "        # 64x64x128 --> 128x128x64\n",
    "        trans_conv4 = tf.layers.conv2d_transpose(inputs = trans_conv3_out,\n",
    "                                  filters = 64,\n",
    "                                  kernel_size = [5,5],\n",
    "                                  strides = [2,2],\n",
    "                                  padding = \"SAME\",\n",
    "                                kernel_initializer=tf.truncated_normal_initializer(stddev=0.02),\n",
    "                                name=\"trans_conv4\")\n",
    "\n",
    "        batch_trans_conv4 = tf.layers.batch_normalization(inputs = trans_conv4, training=is_train, epsilon=1e-5, name=\"batch_trans_conv4\")\n",
    "\n",
    "        trans_conv4_out = tf.nn.leaky_relu(batch_trans_conv4, alpha=alpha, name=\"trans_conv4_out\")\n",
    "\n",
    "\n",
    "        # Transposed conv 5 --> tanh\n",
    "        # 128x128x64 --> 128x128x3\n",
    "        logits = tf.layers.conv2d_transpose(inputs = trans_conv4_out,\n",
    "                                  filters = 3,\n",
    "                                  kernel_size = [5,5],\n",
    "                                  strides = [1,1],\n",
    "                                  padding = \"SAME\",\n",
    "                                kernel_initializer=tf.truncated_normal_initializer(stddev=0.02),\n",
    "                                name=\"logits\")\n",
    "\n",
    "        out = tf.tanh(logits, name=\"out\")\n",
    "\n",
    "        return out\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fbe4a69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 9\n",
    "\n",
    "def discriminator(x, is_reuse=False, alpha = 0.2):\n",
    "    ''' Build the discriminator network.\n",
    "\n",
    "        Arguments\n",
    "        ---------\n",
    "        x : Input tensor for the discriminator\n",
    "        n_units: Number of units in hidden layer\n",
    "        reuse : Reuse the variables with tf.variable_scope\n",
    "        alpha : leak parameter for leaky ReLU\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        out, logits:\n",
    "    '''\n",
    "    with tf.variable_scope(\"discriminator\", reuse = is_reuse):\n",
    "\n",
    "        # Input layer 128*128*3 --> 64x64x64\n",
    "        # Conv --> BatchNorm --> LeakyReLU\n",
    "        conv1 = tf.layers.conv2d(inputs = x,\n",
    "                                filters = 64,\n",
    "                                kernel_size = [5,5],\n",
    "                                strides = [2,2],\n",
    "                                padding = \"SAME\",\n",
    "                                kernel_initializer=tf.truncated_normal_initializer(stddev=0.02),\n",
    "                                name='conv1')\n",
    "\n",
    "        batch_norm1 = tf.layers.batch_normalization(conv1,\n",
    "                                                   training = True,\n",
    "                                                   epsilon = 1e-5,\n",
    "                                                     name = 'batch_norm1')\n",
    "\n",
    "        conv1_out = tf.nn.leaky_relu(batch_norm1, alpha=alpha, name=\"conv1_out\")\n",
    "\n",
    "\n",
    "        # 64x64x64--> 32x32x128\n",
    "        # Conv --> BatchNorm --> LeakyReLU\n",
    "        conv2 = tf.layers.conv2d(inputs = conv1_out,\n",
    "                                filters = 128,\n",
    "                                kernel_size = [5, 5],\n",
    "                                strides = [2, 2],\n",
    "                                padding = \"SAME\",\n",
    "                                kernel_initializer=tf.truncated_normal_initializer(stddev=0.02),\n",
    "                                name='conv2')\n",
    "\n",
    "        batch_norm2 = tf.layers.batch_normalization(conv2,\n",
    "                                                   training = True,\n",
    "                                                   epsilon = 1e-5,\n",
    "                                                     name = 'batch_norm2')\n",
    "\n",
    "        conv2_out = tf.nn.leaky_relu(batch_norm2, alpha=alpha, name=\"conv2_out\")\n",
    "\n",
    "\n",
    "\n",
    "        # 32x32x128 --> 16x16x256\n",
    "        # Conv --> BatchNorm --> LeakyReLU\n",
    "        conv3 = tf.layers.conv2d(inputs = conv2_out,\n",
    "                                filters = 256,\n",
    "                                kernel_size = [5, 5],\n",
    "                                strides = [2, 2],\n",
    "                                padding = \"SAME\",\n",
    "                                kernel_initializer=tf.truncated_normal_initializer(stddev=0.02),\n",
    "                                name='conv3')\n",
    "\n",
    "        batch_norm3 = tf.layers.batch_normalization(conv3,\n",
    "                                                   training = True,\n",
    "                                                   epsilon = 1e-5,\n",
    "                                                name = 'batch_norm3')\n",
    "\n",
    "        conv3_out = tf.nn.leaky_relu(batch_norm3, alpha=alpha, name=\"conv3_out\")\n",
    "\n",
    "\n",
    "\n",
    "        # 16x16x256 --> 16x16x512\n",
    "        # Conv --> BatchNorm --> LeakyReLU\n",
    "        conv4 = tf.layers.conv2d(inputs = conv3_out,\n",
    "                                filters = 512,\n",
    "                                kernel_size = [5, 5],\n",
    "                                strides = [1, 1],\n",
    "                                padding = \"SAME\",\n",
    "                                kernel_initializer=tf.truncated_normal_initializer(stddev=0.02),\n",
    "                                name='conv4')\n",
    "\n",
    "        batch_norm4 = tf.layers.batch_normalization(conv4,\n",
    "                                                   training = True,\n",
    "                                                   epsilon = 1e-5,\n",
    "                                                name = 'batch_norm4')\n",
    "\n",
    "        conv4_out = tf.nn.leaky_relu(batch_norm4, alpha=alpha, name=\"conv4_out\")\n",
    "\n",
    "\n",
    "\n",
    "        # 16x16x512 --> 8x8x1024\n",
    "        # Conv --> BatchNorm --> LeakyReLU\n",
    "        conv5 = tf.layers.conv2d(inputs = conv4_out,\n",
    "                                filters = 1024,\n",
    "                                kernel_size = [5, 5],\n",
    "                                strides = [2, 2],\n",
    "                                padding = \"SAME\",\n",
    "                                kernel_initializer=tf.truncated_normal_initializer(stddev=0.02),\n",
    "                                name='conv5')\n",
    "\n",
    "        batch_norm5 = tf.layers.batch_normalization(conv5,\n",
    "                                                   training = True,\n",
    "                                                   epsilon = 1e-5,\n",
    "                                                name = 'batch_norm5')\n",
    "\n",
    "        conv5_out = tf.nn.leaky_relu(batch_norm5, alpha=alpha, name=\"conv5_out\")\n",
    "\n",
    "\n",
    "        # Flatten it\n",
    "        flatten = tf.reshape(conv5_out, (-1, 8*8*1024))\n",
    "\n",
    "        # Logits\n",
    "        logits = tf.layers.dense(inputs = flatten,\n",
    "                                units = 1,\n",
    "                                activation = None)\n",
    "\n",
    "\n",
    "        out = tf.sigmoid(logits)\n",
    "\n",
    "        return out, logits\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72f65f11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 10\n",
    "def model_loss(input_real, input_z, output_channel_dim, alpha):\n",
    "    \"\"\"\n",
    "    Get the loss for the discriminator and generator\n",
    "    :param input_real: Images from the real dataset\n",
    "    :param input_z: Z input\n",
    "    :param out_channel_dim: The number of channels in the output image\n",
    "    :return: A tuple of (discriminator loss, generator loss)\n",
    "    \"\"\"\n",
    "    # Generator network \n",
    "    g_model = generator(input_z, output_channel_dim)\n",
    "    # g_model is the generator output\n",
    "\n",
    "    # Discriminator network \n",
    "    d_model_real, d_logits_real = discriminator(input_real, alpha=alpha)\n",
    "    d_model_fake, d_logits_fake = discriminator(g_model,is_reuse=True, alpha=alpha)\n",
    "\n",
    "    # Calculate losses\n",
    "    d_loss_real = tf.reduce_mean(\n",
    "                  tf.nn.sigmoid_cross_entropy_with_logits(logits=d_logits_real,\n",
    "                                                          labels=tf.ones_like(d_model_real)))\n",
    "    d_loss_fake = tf.reduce_mean(\n",
    "                  tf.nn.sigmoid_cross_entropy_with_logits(logits=d_logits_fake,\n",
    "                                                          labels=tf.zeros_like(d_model_fake)))\n",
    "    d_loss = d_loss_real + d_loss_fake\n",
    "\n",
    "    g_loss = tf.reduce_mean(\n",
    "             tf.nn.sigmoid_cross_entropy_with_logits(logits=d_logits_fake,\n",
    "                                                     labels=tf.ones_like(d_model_fake)))\n",
    "\n",
    "    return d_loss, g_loss\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b9a3e6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 11\n",
    "def model_optimizers(d_loss, g_loss, lr_D, lr_G, beta1):\n",
    "    \"\"\"\n",
    "    Get optimization operations\n",
    "    :param d_loss: Discriminator loss Tensor\n",
    "    :param g_loss: Generator loss Tensor\n",
    "    :param learning_rate: Learning Rate Placeholder\n",
    "    :param beta1: The exponential decay rate for the 1st moment in the optimizer\n",
    "    :return: A tuple of (discriminator training operation, generator training operation)\n",
    "    \"\"\"\n",
    "    # Get the trainable_variables, split into G and D parts\n",
    "    t_vars = tf.trainable_variables()\n",
    "    g_vars = [var for var in t_vars if var.name.startswith(\"generator\")]\n",
    "    d_vars = [var for var in t_vars if var.name.startswith(\"discriminator\")]\n",
    "\n",
    "    update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "\n",
    "    # Generator update\n",
    "    gen_updates = [op for op in update_ops if op.name.startswith('generator')]\n",
    "\n",
    "    # Optimizers\n",
    "    with tf.control_dependencies(gen_updates):\n",
    "        d_train_opt = tf.train.AdamOptimizer(learning_rate=lr_D, beta1=beta1).minimize(d_loss, var_list=d_vars)\n",
    "        g_train_opt = tf.train.AdamOptimizer(learning_rate=lr_G, beta1=beta1).minimize(g_loss, var_list=g_vars)\n",
    "\n",
    "    return d_train_opt, g_train_opt\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24385b9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# step 12\n",
    "def show_generator_output(sess, n_images, input_z, out_channel_dim, image_mode, image_path, save, show):\n",
    "    \"\"\"\n",
    "    Show example output for the generator\n",
    "    :param sess: TensorFlow session\n",
    "    :param n_images: Number of Images to display\n",
    "    :param input_z: Input Z Tensor\n",
    "    :param out_channel_dim: The number of channels in the output image\n",
    "    :param image_mode: The mode to use for images (\"RGB\" or \"L\")\n",
    "    :param image_path: Path to save the image\n",
    "    \"\"\"\n",
    "    cmap = None if image_mode == 'RGB' else 'gray'\n",
    "    z_dim = input_z.get_shape().as_list()[-1]\n",
    "    example_z = np.random.uniform(-1, 1, size=[n_images, z_dim])\n",
    "\n",
    "    samples = sess.run(\n",
    "        generator(input_z, out_channel_dim, False),\n",
    "        feed_dict={input_z: example_z})\n",
    "\n",
    "    images_grid = images_square_grid(samples, image_mode)\n",
    "\n",
    "    if save == True:\n",
    "        # Create the directory if it doesn't exist\n",
    "        os.makedirs(os.path.dirname(image_path), exist_ok=True)\n",
    "        # Save image\n",
    "        images_grid.save(image_path, 'JPEG')\n",
    "\n",
    "    if show == True:\n",
    "        plt.imshow(images_grid, cmap=cmap)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8e74c68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 13\n",
    "config = tf.compat.v1.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02fb2fcf",
   "metadata": {},
   "outputs": [],
   "source": [
    " # Step 14\n",
    "def train(epoch_count, batch_size, z_dim, learning_rate_D, learning_rate_G, beta1, get_batches, data_shape, data_image_mode, alpha):\n",
    "    \"\"\"\n",
    "    Train the GAN\n",
    "    :param epoch_count: Number of epochs\n",
    "    :param batch_size: Batch Size\n",
    "    :param z_dim: Z dimension\n",
    "    :param learning_rate: Learning Rate\n",
    "    :param beta1: The exponential decay rate for the 1st moment in the optimizer\n",
    "    :param get_batches: Function to get batches\n",
    "    :param data_shape: Shape of the data\n",
    "    :param data_image_mode: The image mode to use for images (\"RGB\" or \"L\")\n",
    "    \"\"\"\n",
    "    # Create our input placeholders\n",
    "    input_images, input_z, lr_G, lr_D = model_inputs(data_shape[1:], z_dim)\n",
    "\n",
    "    # Losses\n",
    "    d_loss, g_loss = model_loss(input_images, input_z, data_shape[3], alpha)\n",
    "\n",
    "    # Optimizers\n",
    "    d_opt, g_opt = model_optimizers(d_loss, g_loss, lr_D, lr_G, beta1)\n",
    "\n",
    "    g_losses = []\n",
    "    d_losses = []\n",
    "\n",
    "    version = \"firstTrain\"\n",
    "    with tf.Session(config=config) as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "\n",
    "        # Saver\n",
    "        saver = tf.train.Saver()\n",
    "\n",
    "        num_epoch = 0\n",
    "\n",
    "        if from_checkpoint == True:\n",
    "            saver.restore(sess, \"./models/model.ckpt-300\")\n",
    "            image_path = \"new_train/new_gen_image.jpg\"\n",
    "            show_generator_output(sess, 1, input_z, data_shape[3], data_image_mode, image_path, True, True)\n",
    "\n",
    "        else:\n",
    "            for epoch_i in range(epoch_count):\n",
    "                num_epoch += 1\n",
    "                if num_epoch % 5 == 0:\n",
    "                    save_path = saver.save(sess, \"./models/model.ckpt\")\n",
    "                    print(\"Model saved\")\n",
    "\n",
    "                # saves model every 50 epochs\n",
    "                if epoch_i > 50 and epoch_i % 50 == 0:\n",
    "                    save_path = saver.save(sess, \"./models/model.ckpt\", global_step = epoch_i, write_meta_graph=False)\n",
    "                for batch_images in get_batches(batch_size):\n",
    "                    # Random noise\n",
    "                    batch_z = np.random.uniform(-1, 1, size=(batch_size, z_dim))\n",
    "                    # Run optimizers\n",
    "                    _ = sess.run(d_opt, feed_dict={input_images: batch_images, input_z: batch_z, lr_D: learning_rate_D})\n",
    "                    _ = sess.run(g_opt, feed_dict={input_images: batch_images, input_z: batch_z, lr_G: learning_rate_G})\n",
    "\n",
    "                # will calculate losses and generate an image for each epoch\n",
    "\n",
    "                train_loss_d = d_loss.eval({input_z: batch_z, input_images: batch_images})\n",
    "                train_loss_g = g_loss.eval({input_z: batch_z})\n",
    "                g_losses.append(train_loss_g)\n",
    "                d_losses.append(train_loss_d)\n",
    "                # Save it\n",
    "                image_name = str(epoch_i) + \".jpg\"\n",
    "                image_path = \"./images/\" + image_name\n",
    "                print(\"Epoch {}/{}...\".format(epoch_i+1, epochs),\n",
    "                      \"Discriminator Loss: {:.4f}...\".format(train_loss_d),\n",
    "                      \"Generator Loss: {:.4f}\".format(train_loss_g))\n",
    "                show_generator_output(sess, 9, input_z, data_shape[3], data_image_mode, image_path, True, True)\n",
    "\n",
    "    return d_losses, g_losses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26840a4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Step 15\n",
    "# Size input image for discriminator\n",
    "real_size = (128,128,3)\n",
    "\n",
    "# Size of latent vector to generator\n",
    "z_dim = 100\n",
    "learning_rate_D =  .00005 # Thanks to Alexia Jolicoeur Martineau https://ajolicoeur.wordpress.com/cats/\n",
    "learning_rate_G = 2e-4 # Thanks to Alexia Jolicoeur Martineau https://ajolicoeur.wordpress.com/cats/\n",
    "batch_size = 32\n",
    "epochs = 1200\n",
    "alpha = 0.5\n",
    "beta1 = 0.5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ab9b1ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 16\n",
    "dataset = Dataset(resized_data_filenames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d0e4ae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 17\n",
    "dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc68938e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 18\n",
    "with tf.Graph().as_default():\n",
    "    d_losses, g_losses = train(epochs, batch_size, z_dim, learning_rate_D, learning_rate_G, beta1, dataset.get_batches, dataset.shape, dataset.image_mode, alpha)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58f7b0f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 19\n",
    "fig, ax = plt.subplots()\n",
    "d_losses = np.array(d_losses)\n",
    "g_losses = np.array(g_losses)\n",
    "plt.plot(d_losses, label='Discriminator', alpha=0.5)\n",
    "plt.plot(g_losses, label='Generator', alpha=0.5)\n",
    "plt.title(\"Training Losses\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aa509e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 20\n",
    "from_checkpoint = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c831abe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 21\n",
    "with tf.Graph().as_default():\n",
    "    train(epochs, batch_size, z_dim, learning_rate_D, learning_rate_G, beta1, dataset.get_batches,\n",
    "          dataset.shape, dataset.image_mode, alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bc0d5a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the generate_video function\n",
    "import cv2\n",
    "import numpy as np\n",
    "import tensorflow.compat.v1 as tf\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "# Disable eager execution to work with TensorFlow 1.x style code\n",
    "tf.disable_eager_execution()\n",
    "\n",
    "def generate_video(sess, generator_tensor, input_z, latent_dim, video_output_path, num_frames=100, fps=30):\n",
    "    \"\"\"\n",
    "    Generate a video using the trained generator model.\n",
    "\n",
    "    Args:\n",
    "        sess: TensorFlow session.\n",
    "        generator_tensor: Output tensor of the generator.\n",
    "        input_z: Input placeholder tensor for the generator.\n",
    "        latent_dim: Dimension of the latent space for noise generation.\n",
    "        video_output_path: Path to save the generated video.\n",
    "        num_frames: Number of frames to generate for the video (default: 100).\n",
    "        fps: Frames per second for the generated video (default: 30).\n",
    "    \"\"\"\n",
    "    # Generate random latent noise vectors\n",
    "    latent_vectors = np.random.normal(0, 1, size=(num_frames, latent_dim))\n",
    "\n",
    "    # Generate frames\n",
    "    frames = []\n",
    "    for latent_vector in latent_vectors:\n",
    "        # Generate image from latent vector using the model\n",
    "        generated_image = sess.run(generator_tensor, feed_dict={input_z: latent_vector.reshape(1, latent_dim)})[0]\n",
    "        # Scale the pixel values to 0-255\n",
    "        generated_image = ((generated_image + 1) * 127.5).astype(np.uint8)\n",
    "        frames.append(generated_image)\n",
    "\n",
    "    # Get height, width, and channels of the first frame\n",
    "    height, width, channels = frames[0].shape\n",
    "\n",
    "    # Initialize video writer\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')  # Codec for MP4\n",
    "    video_writer = cv2.VideoWriter(video_output_path, fourcc, fps, (width, height))\n",
    "\n",
    "    # Write frames to video\n",
    "    for frame in frames:\n",
    "        video_writer.write(cv2.cvtColor(frame, cv2.COLOR_RGB2BGR))  # Convert to BGR for OpenCV\n",
    "\n",
    "    # Release video writer\n",
    "    video_writer.release()\n",
    "    print(f'Video saved to {video_output_path}')\n",
    "\n",
    "# Step 20\n",
    "from_checkpoint = True\n",
    "\n",
    "# Step 21\n",
    "with tf.Graph().as_default():\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "\n",
    "        # Restore the trained generator model\n",
    "        saver = tf.train.import_meta_graph('./models/model.ckpt.meta')\n",
    "        saver.restore(sess, './models/model.ckpt')\n",
    "\n",
    "        # Get the generator tensor by name\n",
    "        generator_tensor_name = 'generator_499/out:0'  # Update this line with the correct tensor name\n",
    "        generator_tensor = sess.graph.get_tensor_by_name(generator_tensor_name)\n",
    "\n",
    "        # Define the input placeholder tensor by name\n",
    "        input_z = sess.graph.get_tensor_by_name('input_z:0')\n",
    "\n",
    "        # Generate the video using the trained generator model\n",
    "        video_output_path = 'generated_video.mp4'\n",
    "        num_frames = 100  # Number of frames to generate\n",
    "        fps = 60  # Frames per second for the video\n",
    "        latent_dim = 100  # Dimension of the latent space for noise generation\n",
    "\n",
    "        generate_video(sess, generator_tensor, input_z, latent_dim, video_output_path, num_frames, fps)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02feb9b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.compat.v1 as tf\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "tf.disable_eager_execution()\n",
    "\n",
    "def load_generator_model(sess, checkpoint_path):\n",
    "    saver = tf.train.import_meta_graph(checkpoint_path + '.meta')\n",
    "    saver.restore(sess, checkpoint_path)\n",
    "\n",
    "    input_image = sess.graph.get_tensor_by_name('input_z:0')\n",
    "    generated_image = sess.graph.get_tensor_by_name('generator_499/out:0')\n",
    "\n",
    "    return input_image, generated_image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c5ed4dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_image(image_path, image_size):\n",
    "    image = cv2.imread(image_path)\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    image = cv2.resize(image, (image_size, image_size))\n",
    "    image = (image / 127.5) - 1.0  # Normalize to [-1, 1]\n",
    "    return image[np.newaxis, ...]  # Add batch dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b0e2e4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_image(sess, input_image_tensor, generated_image_tensor, input_image):\n",
    "    generated_image = sess.run(generated_image_tensor, feed_dict={input_image_tensor: input_image})\n",
    "    generated_image = (generated_image[0] + 1) * 127.5  # Rescale to [0, 255]\n",
    "    generated_image = generated_image.astype(np.uint8)\n",
    "    return generated_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b2d5d89",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_image(image, output_path):\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "    cv2.imwrite(output_path, image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75b05219",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  GENERATE AN IMAGE FROM AN INPUT GIVEN (Not using it, just to see image outputs)\n",
    "def main(input_image_path, output_image_path, checkpoint_path, image_size, latent_dim):\n",
    "    with tf.Graph().as_default():\n",
    "        with tf.Session() as sess:\n",
    "            input_latent_tensor, generated_image_tensor = load_generator_model(sess, checkpoint_path)\n",
    "\n",
    "            # Generate random latent vector instead of processing an input image\n",
    "            input_latent = np.random.normal(0, 1, size=(1, latent_dim))\n",
    "            generated_image = generate_image(sess, input_latent_tensor, generated_image_tensor, input_latent)\n",
    "\n",
    "            save_image(generated_image, output_image_path)\n",
    "            print(f\"Generated image saved to {output_image_path}\")\n",
    "\n",
    "# Parameters\n",
    "input_image_path = '/content/drive/MyDrive/photos2/my image.jpeg'  # Path to the input image\n",
    "output_image_path = '/content/drive/MyDrive/photos2/my image2.jpeg'  # Path to save the generated image\n",
    "checkpoint_path = './models/model.ckpt'  # Path to the model checkpoint\n",
    "image_size = 128  # Example image size (adjust as needed)\n",
    "latent_dim = 100  # Dimension of the latent space for noise generation\n",
    "\n",
    "main(input_image_path, output_image_path, checkpoint_path, image_size, latent_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "999fa27d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GENERATE VIDEO FROM INPUT IMAGE GENERTED GANS \n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import tensorflow.compat.v1 as tf\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "# Disable eager execution to work with TensorFlow 1.x style code\n",
    "tf.disable_eager_execution()\n",
    "\n",
    "def preprocess_image(image_path, image_size):\n",
    "    image = cv2.imread(image_path)\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    image = cv2.resize(image, (image_size, image_size))\n",
    "    image = (image / 127.5) - 1.0  # Normalize to [-1, 1]\n",
    "    return image[np.newaxis, ...]  # Add batch dimension\n",
    "\n",
    "def generate_video(sess, generator_tensor, input_z, latent_dim, video_output_path, input_image, num_frames=100, fps=30):\n",
    "    \"\"\"\n",
    "    Generate a video using the trained generator model.\n",
    "\n",
    "    Args:\n",
    "        sess: TensorFlow session.\n",
    "        generator_tensor: Output tensor of the generator.\n",
    "        input_z: Input placeholder tensor for the generator.\n",
    "        latent_dim: Dimension of the latent space for noise generation.\n",
    "        video_output_path: Path to save the generated video.\n",
    "        input_image: Preprocessed input image.\n",
    "        num_frames: Number of frames to generate for the video (default: 100).\n",
    "        fps: Frames per second for the generated video (default: 30).\n",
    "    \"\"\"\n",
    "    # Generate random latent noise vectors\n",
    "    latent_vectors = np.random.normal(0, 1, size=(num_frames, latent_dim))\n",
    "\n",
    "    # Combine latent vectors with the input image in some way (this is an example and might need adjustment)\n",
    "    input_image_flat = input_image.flatten()\n",
    "    if len(input_image_flat) >= latent_dim:\n",
    "        input_image_flat = input_image_flat[:latent_dim]\n",
    "    else:\n",
    "        input_image_flat = np.pad(input_image_flat, (0, latent_dim - len(input_image_flat)), 'constant')\n",
    "\n",
    "    latent_vectors = latent_vectors + input_image_flat\n",
    "\n",
    "    # Generate frames\n",
    "    frames = []\n",
    "    for latent_vector in latent_vectors:\n",
    "        # Generate image from latent vector using the model\n",
    "        generated_image = sess.run(generator_tensor, feed_dict={input_z: latent_vector.reshape(1, latent_dim)})[0]\n",
    "        # Scale the pixel values to 0-255\n",
    "        generated_image = ((generated_image + 1) * 127.5).astype(np.uint8)\n",
    "        frames.append(generated_image)\n",
    "\n",
    "    # Get height, width, and channels of the first frame\n",
    "    height, width, channels = frames[0].shape\n",
    "\n",
    "    # Initialize video writer\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')  # Codec for MP4\n",
    "    video_writer = cv2.VideoWriter(video_output_path, fourcc, fps, (width, height))\n",
    "\n",
    "    # Write frames to video\n",
    "    for frame in frames:\n",
    "        video_writer.write(cv2.cvtColor(frame, cv2.COLOR_RGB2BGR))  # Convert to BGR for OpenCV\n",
    "\n",
    "    # Release video writer\n",
    "    video_writer.release()\n",
    "    print(f'Video saved to {video_output_path}')\n",
    "\n",
    "\n",
    "# Step 21\n",
    "with tf.Graph().as_default():\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "\n",
    "        # Restore the trained generator model\n",
    "        saver = tf.train.import_meta_graph('./models/model.ckpt.meta')\n",
    "        saver.restore(sess, './models/model.ckpt')\n",
    "\n",
    "        # Get the generator tensor by name\n",
    "        generator_tensor_name = 'generator_499/out:0'  # Update this line with the correct tensor name\n",
    "        generator_tensor = sess.graph.get_tensor_by_name(generator_tensor_name)\n",
    "\n",
    "        # Define the input placeholder tensor by name\n",
    "        input_z = sess.graph.get_tensor_by_name('input_z:0')\n",
    "\n",
    "        # Preprocess the input image\n",
    "        input_image_path = '/content/drive/MyDrive/photos2/image.jpeg'\n",
    "        image_size = 128  # Example image size (adjust as needed)\n",
    "        input_image = preprocess_image(input_image_path, image_size)\n",
    "\n",
    "        # Generate the video using the trained generator model\n",
    "        video_output_path = 'generated_video.mp4'\n",
    "        num_frames = 216  # Number of frames to generate\n",
    "        fps = 3  # Frames per second for the video\n",
    "        latent_dim = 100  # Dimension of the latent space for noise generation\n",
    "\n",
    "        generate_video(sess, generator_tensor, input_z, latent_dim, video_output_path, input_image, num_frames, fps)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7746f9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Processing midi file\n",
    "from collections import defaultdict\n",
    "from mido import MidiFile\n",
    "from pydub import AudioSegment\n",
    "from pydub.generators import Sine\n",
    "\n",
    "#Segment audio files (retain a specific loop)\n",
    "from pydub import AudioSegment\n",
    "import tempfile\n",
    "import os\n",
    "\n",
    "#Playing generated audios\n",
    "import IPython.display as ipd\n",
    "\n",
    "#Combine audios\n",
    "import soundfile as sf\n",
    "import numpy as np\n",
    "import resampy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "671d25d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "midi_file_path = \"/content/drive/MyDrive/audios/Never-Gonna-Give-You-Up-3.mid\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4697f576",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ticks_to_ms(ticks, tempo, ticks_per_beat):\n",
    "    \"\"\"Convert MIDI ticks to milliseconds.\"\"\"\n",
    "    beats_per_second = tempo / 60.0\n",
    "    seconds_per_beat = 1 / beats_per_second\n",
    "    seconds_per_tick = seconds_per_beat / ticks_per_beat\n",
    "    return ticks * seconds_per_tick * 1000\n",
    "\n",
    "def note_to_freq(note):\n",
    "    \"\"\"Convert MIDI note number to frequency.\"\"\"\n",
    "    return 440.0 * (2.0 ** ((note - 69) / 12.0))\n",
    "\n",
    "def process_midi_file(midi_file_path, tempo=100):\n",
    "    \"\"\"\n",
    "    Process a MIDI file and generate a melody in WAV format.\n",
    "\n",
    "    This function reads a MIDI file, converts the note events into audio signals,\n",
    "    and generates a WAV file that plays the melody. It uses sine wave generators\n",
    "    to synthesize the audio signals for each note.\n",
    "\n",
    "    Parameters:\n",
    "        midi_file_path (str): Path to the input MIDI file.\n",
    "        tempo (int): Tempo of the generated audio in beats per minute (BPM). Default is 100 BPM.\n",
    "\n",
    "    Notes:\n",
    "        - The function assumes that all note events in the MIDI file are to be converted\n",
    "          to sine wave audio signals.\n",
    "        - The generated WAV file will have the same duration as the input MIDI file.\n",
    "        - A maximum of 1000 iterations per track is used to prevent infinite loops.\n",
    "        - The function overlays the generated audio signals on a silent audio segment\n",
    "          of the same length as the MIDI file.\n",
    "    \"\"\"\n",
    "    # Load the MIDI file\n",
    "    mid = MidiFile(midi_file_path)\n",
    "    print(f\"Loaded MIDI file '{midi_file_path}' with length {mid.length:.2f} seconds\")\n",
    "\n",
    "    # Create a silent audio segment of the same length as the MIDI file\n",
    "    output = AudioSegment.silent(duration=mid.length * 1000.0)\n",
    "    print(f\"Created silent audio segment of duration {mid.length * 1000.0:.2f} ms\")\n",
    "\n",
    "    max_iterations = 1000  # Maximum number of iterations to prevent infinite loops\n",
    "    \n",
    "    print(\"Processing track\")\n",
    "    \n",
    "    for i, track in enumerate(mid.tracks):\n",
    "        current_pos = 0.0\n",
    "        current_notes = defaultdict(dict)\n",
    "        iterations = 0\n",
    "\n",
    "        for msg in track:\n",
    "            if iterations >= max_iterations:\n",
    "                break\n",
    "\n",
    "            current_pos += ticks_to_ms(msg.time, tempo, mid.ticks_per_beat)\n",
    "\n",
    "            if msg.type == 'note_on' and msg.velocity > 0:\n",
    "                current_notes[msg.channel][msg.note] = (current_pos, msg)\n",
    "\n",
    "            if msg.type == 'note_off' or (msg.type == 'note_on' and msg.velocity == 0):\n",
    "                start_pos, start_msg = current_notes[msg.channel].pop(msg.note, (None, None))\n",
    "                if start_pos is None:\n",
    "                    continue\n",
    "\n",
    "                duration = current_pos - start_pos\n",
    "\n",
    "                if duration > 50:  # Ensure duration is valid\n",
    "                    signal_generator = Sine(note_to_freq(msg.note))\n",
    "                    rendered = signal_generator.to_audio_segment(duration=int(duration - 50), volume=-20).fade_out(100).fade_in(30)\n",
    "                    output = output.overlay(rendered, start_pos)\n",
    "\n",
    "            iterations += 1\n",
    "\n",
    "    print(f\"Processing complete\")\n",
    "    \n",
    "    # Export the truncated audio to a temporary file\n",
    "    temp_output_path = tempfile.NamedTemporaryFile(suffix='.wav', delete=False).name\n",
    "    output.export(temp_output_path, format='wav')\n",
    "    \n",
    "    print(f\"Wav audio file saved at: {temp_output_path}\")\n",
    "    \n",
    "    return temp_output_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15ca0439",
   "metadata": {},
   "outputs": [],
   "source": [
    "wav_audio_path = process_midi_file(midi_file_path, tempo=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46474113",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Play the wav audio to test it and determine where to loop it\n",
    "ipd.display(ipd.Audio(wav_audio_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c08c85c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_wav_output(midi_file_path, tempo=100):\n",
    "    \"\"\"\n",
    "    Process a MIDI file and generate a melody in WAV format.\n",
    "\n",
    "    Parameters:\n",
    "        midi_file_path (str): Path to the input MIDI file.\n",
    "        tempo (int): Tempo of the generated audio in beats per minute (BPM). Default is 100 BPM.\n",
    "\n",
    "    Returns:\n",
    "        AudioSegment: The generated audio segment.\n",
    "    \"\"\"\n",
    "    # Load the MIDI file\n",
    "    mid = MidiFile(midi_file_path)\n",
    "    print(f\"Loaded MIDI file '{midi_file_path}' with length {mid.length:.2f} seconds\")\n",
    "\n",
    "    # Create a silent audio segment of the same length as the MIDI file\n",
    "    output = AudioSegment.silent(duration=mid.length * 1000.0)\n",
    "    print(f\"Created silent audio segment of duration {mid.length * 1000.0:.2f} ms\")\n",
    "\n",
    "    max_iterations = 1000  # Maximum number of iterations to prevent infinite loops\n",
    "    \n",
    "    print(\"Processing track\")\n",
    "    \n",
    "    for i, track in enumerate(mid.tracks):\n",
    "        current_pos = 0.0\n",
    "        current_notes = defaultdict(dict)\n",
    "        iterations = 0\n",
    "\n",
    "        for msg in track:\n",
    "            if iterations >= max_iterations:\n",
    "                break\n",
    "\n",
    "            current_pos += ticks_to_ms(msg.time, tempo, mid.ticks_per_beat)\n",
    "\n",
    "            if msg.type == 'note_on' and msg.velocity > 0:\n",
    "                current_notes[msg.channel][msg.note] = (current_pos, msg)\n",
    "\n",
    "            if msg.type == 'note_off' or (msg.type == 'note_on' and msg.velocity == 0):\n",
    "                start_pos, start_msg = current_notes[msg.channel].pop(msg.note, (None, None))\n",
    "                if start_pos is None:\n",
    "                    continue\n",
    "\n",
    "                duration = current_pos - start_pos\n",
    "\n",
    "                if duration > 50:  # Ensure duration is valid\n",
    "                    signal_generator = Sine(note_to_freq(msg.note))\n",
    "                    rendered = signal_generator.to_audio_segment(duration=int(duration - 50), volume=-20).fade_out(100).fade_in(30)\n",
    "                    output = output.overlay(rendered, start_pos)\n",
    "\n",
    "            iterations += 1\n",
    "\n",
    "    print(f\"Processing complete\")\n",
    "    \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1a437f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_wav_output(midi_file_path, output_file_path):\n",
    "    \"\"\"\n",
    "    Process a MIDI file and save the generated audio to a local file.\n",
    "\n",
    "    This function calls the process_midi_file function to convert the MIDI file\n",
    "    into an audio segment and then saves the audio segment as a WAV file at the\n",
    "    specified output path.\n",
    "\n",
    "    Parameters:\n",
    "        midi_file_path (str): Path to the input MIDI file.\n",
    "        output_file_path (str): Path to save the output WAV file.\n",
    "\n",
    "    Example:\n",
    "        save_to_local(\"collab.mid\", \"output.wav\")\n",
    "    \"\"\"\n",
    "    # Call the process_midi_file function\n",
    "    # Call the function and capture the second returned value as wav_output\n",
    "    wav_output = generate_wav_output(midi_file_path)\n",
    "\n",
    "    \n",
    "    # Export the returned AudioSegment object to a file\n",
    "    wav_output.export(output_file_path, format=\"wav\")\n",
    "    print(f\"Wav Audio file saved as '{output_file_path}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e927fb20",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save the output file\n",
    "output_file_path = \"first_melody_output.wav\"\n",
    "save_wav_output(midi_file_path, output_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e80cc3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def truncate_wav(start_time_sec, end_time_sec):\n",
    "    \"\"\"\n",
    "    Truncate a WAV file from start_time_sec to end_time_sec and return the truncated audio.\n",
    "\n",
    "    Parameters:\n",
    "        start_time_sec (float): Start time in seconds.\n",
    "        end_time_sec (float): End time in seconds.\n",
    "\n",
    "    Returns:\n",
    "        AudioSegment: The truncated audio segment.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Process the MIDI file to generate the WAV audio segment\n",
    "    wav_output = generate_wav_output(midi_file_path)\n",
    "\n",
    "    # Calculate start and end times in milliseconds\n",
    "    start_time_ms = start_time_sec * 1000\n",
    "    end_time_ms = end_time_sec * 1000\n",
    "\n",
    "    # Extract the segment\n",
    "    truncated_audio = wav_output[start_time_ms:end_time_ms]\n",
    "\n",
    "    print(f\"Truncation complete. Segment from {start_time_sec} to {end_time_sec} seconds.\")\n",
    "\n",
    "    # Export the truncated audio to a temporary file\n",
    "    temp_output_path = tempfile.NamedTemporaryFile(suffix='.wav', delete=False).name\n",
    "    truncated_audio.export(temp_output_path, format='wav')\n",
    "    \n",
    "    print(f\"Truncated audio file saved at: {temp_output_path}\")\n",
    "    \n",
    "    return temp_output_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c30789cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Usage of truncate_wav function\n",
    "start_time_sec = 7\n",
    "end_time_sec = 31\n",
    "truncated_audio_path = truncate_wav(start_time_sec, end_time_sec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a75489e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Play the truncated audio to test it\n",
    "ipd.display(ipd.Audio(truncated_audio_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cf703fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_truncated_wav(start_time_sec, end_time_sec):\n",
    "    \"\"\"\n",
    "    Truncate a WAV file from start_time_sec to end_time_sec and return the truncated audio.\n",
    "\n",
    "    Parameters:\n",
    "        start_time_sec (float): Start time in seconds.\n",
    "        end_time_sec (float): End time in seconds.\n",
    "\n",
    "    Returns:\n",
    "        AudioSegment: The truncated audio segment.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Process the MIDI file to generate the WAV audio segment\n",
    "    wav_output = generate_wav_output(midi_file_path)\n",
    "\n",
    "    # Calculate start and end times in milliseconds\n",
    "    start_time_ms = start_time_sec * 1000\n",
    "    end_time_ms = end_time_sec * 1000\n",
    "\n",
    "    # Extract the segment\n",
    "    truncated_audio = wav_output[start_time_ms:end_time_ms]\n",
    "    return truncated_audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "703fcf57",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_truncated(output_file_path):\n",
    "    \"\"\"\n",
    "    Truncate a WAV file and save the truncated audio to a local file.\n",
    "\n",
    "    This function calls the truncate_wav function to truncate the WAV file and then\n",
    "    saves the truncated audio segment as a WAV file at the specified output path.\n",
    "\n",
    "    Parameters:\n",
    "        output_wav_path (str): Path to save the truncated output WAV file.\n",
    "\n",
    "    Example:\n",
    "        save_truncated(output_file_path)\n",
    "    \"\"\"\n",
    "    # Call the process_midi_file function\n",
    "    truncated_audio = generate_truncated_wav(start_time_sec, end_time_sec)\n",
    "    \n",
    "    # Export the returned AudioSegment object to a file\n",
    "    truncated_audio.export(output_file_path, format=\"wav\")\n",
    "    print(f\"Truncated audio file saved as '{output_file_path}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1376cf3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Usage of the save_truncated function\n",
    "output_file_path = \"truncated_melody.wav\"\n",
    "\n",
    "save_truncated(output_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28ccf64e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loop_truncated_wav(input_wav_path, loop_count):\n",
    "    \"\"\"\n",
    "    Loop a wav file a specified number of times to make one longer file.\n",
    "\n",
    "    :param input_wav_path: Path to the input wav file\n",
    "    :param loop_count: Number of times to loop the wav file\n",
    "    :return: Path to the looped audio file\n",
    "    \"\"\"\n",
    "    # Load the audio file\n",
    "    audio = AudioSegment.from_wav(input_wav_path)\n",
    "\n",
    "    # Loop the audio file\n",
    "    looped_audio = audio * loop_count\n",
    "    \n",
    "    # Export the looped audio to a temporary file\n",
    "    temp_output_path = tempfile.NamedTemporaryFile(suffix='.wav', delete=False).name\n",
    "    looped_audio.export(temp_output_path, format='wav')\n",
    "    \n",
    "    return temp_output_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "859b3330",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "input_wav_path = 'truncated_melody.wav'\n",
    "loop_count = 7  # Number of times to loop the audio\n",
    "\n",
    "looped_audio_path = loop_truncated_wav(input_wav_path, loop_count)\n",
    "print(f\"Looped audio file saved at: {looped_audio_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50227478",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Play the looped audio using IPython display\n",
    "ipd.display(ipd.Audio(looped_audio_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7dae574",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_save_looped_wav(input_wav_path, loop_count, output_file_path):\n",
    "    \"\"\"\n",
    "    Loop a wav file a specified number of times to make one longer file and save.\n",
    "\n",
    "    :param input_wav_path: Path to the input wav file\n",
    "    :param loop_count: Number of times to loop the wav file\n",
    "    :return: Path to the looped audio file\n",
    "    \"\"\"\n",
    "    # Load the audio file\n",
    "    audio = AudioSegment.from_wav(input_wav_path)\n",
    "\n",
    "    # Loop the audio file\n",
    "    looped_audio = audio * loop_count\n",
    "    \n",
    "    # Export the looped audio to a file\n",
    "    looped_audio.export(output_file_path, format=\"wav\")\n",
    "    print(f\"Looped audio file saved as '{output_file_path}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "548d5131",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_file_path = \"looped_audio.wav\"\n",
    "input_wav_path = \"/content/truncated_melody.wav\"\n",
    "generate_save_looped_wav(input_wav_path, 3, output_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47ef2291",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_wav(file_path):\n",
    "    \"\"\"\n",
    "    Reads a WAV file and converts stereo signals to mono.\n",
    "\n",
    "    Parameters:\n",
    "        file_path (str): The path to the WAV file.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing the sample rate and the audio signal as numpy array.\n",
    "    \"\"\"\n",
    "    audio_signal, sample_rate = sf.read(file_path, dtype='int16')\n",
    "    if len(audio_signal.shape) == 2:  # if stereo, convert to mono\n",
    "        audio_signal = audio_signal.mean(axis=1).astype(np.int16)\n",
    "    return sample_rate, audio_signal\n",
    "    print(f\"Completed reading {file_path} and converted to mono signal.\")\n",
    "    \n",
    "def write_wav(file_path, sample_rate, audio_signal):\n",
    "    \"\"\"\n",
    "    Writes audio data to a WAV file.\n",
    "\n",
    "    Parameters:\n",
    "        file_path (str): The path to save the WAV file.\n",
    "        sample_rate (int): The sample rate of the audio.\n",
    "        audio_signal (numpy.ndarray): The audio signal as a numpy array.\n",
    "    \"\"\"\n",
    "    sf.write(file_path, audio_signal, sample_rate, subtype='PCM_16')\n",
    "    \n",
    "    print(f\"Completed writing audio data to a WAV file.\")\n",
    "\n",
    "def resample_audio(audio_signal, original_sr, target_sr):\n",
    "    \"\"\"\n",
    "    Resamples the audio signal to the target sample rate.\n",
    "\n",
    "    Parameters:\n",
    "        audio_signal (numpy.ndarray): The audio signal to resample.\n",
    "        original_sr (int): The original sample rate.\n",
    "        target_sr (int): The target sample rate.\n",
    "\n",
    "    Returns:\n",
    "        numpy.ndarray: The resampled audio signal.\n",
    "    \"\"\"\n",
    "    return resampy.resample(audio_signal.astype(np.float32), original_sr, target_sr).astype(np.int16)\n",
    "    \n",
    "    print(f\"Completed resampling the audio signal to the target sample rate.\")\n",
    "\n",
    "def blend_wav_files(file_paths, output_path, weights=None):\n",
    "    \"\"\"\n",
    "    Blends multiple WAV files into one by averaging them with optional weights.\n",
    "\n",
    "    Parameters:\n",
    "        file_paths (list): List of paths to input WAV files.\n",
    "        output_path (str): The path to save the blended WAV file.\n",
    "        weights (list, optional): List of blending weights for each input file. Default is None.\n",
    "\n",
    "    Raises:\n",
    "        ValueError: If input WAV files have different sample rates.\n",
    "\n",
    "    Note:\n",
    "        The length of the output signal will be the length of the longest input signal.\n",
    "        If weights are not provided, equal weights are used for blending.\n",
    "    \"\"\"\n",
    "    sample_rate, audio_signals = [], []\n",
    "\n",
    "    for file_path in file_paths:\n",
    "        sr, audio = read_wav(file_path)\n",
    "        sample_rate.append(sr)\n",
    "        audio_signals.append(audio)\n",
    "\n",
    "    # Resample all audio signals to the sample rate of the first file\n",
    "    target_sr = sample_rate[0]\n",
    "    audio_signals = [resample_audio(audio, sr, target_sr) if sr != target_sr else audio for audio, sr in zip(audio_signals, sample_rate)]\n",
    "\n",
    "    max_length = max(len(audio) for audio in audio_signals)\n",
    "    blended_signal = np.zeros(max_length)\n",
    "\n",
    "    if weights is None:\n",
    "        weights = [1.0 / len(audio_signals)] * len(audio_signals)\n",
    "\n",
    "    for audio, weight in zip(audio_signals, weights):\n",
    "        padded_audio = np.zeros(max_length)\n",
    "        padded_audio[:len(audio)] = audio\n",
    "        blended_signal += padded_audio * weight\n",
    "\n",
    "    blended_signal = np.clip(blended_signal, -32768, 32767).astype(np.int16)\n",
    "\n",
    "    write_wav(output_path, target_sr, blended_signal)\n",
    "    print(f\"Completed blending multiple WAV files into one. File saved as {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e325736",
   "metadata": {},
   "outputs": [],
   "source": [
    "# melody blender \n",
    "file_paths = ['/content/drive/MyDrive/audios/audio1.wav', '/content/drive/MyDrive/audios/audio2.wav', '/content/drive/MyDrive/audios/audio3.wav', '/content/looped_audio.wav']\n",
    "output_path = \"melody_for_video.wav\"\n",
    "blend_wav_files(file_paths, output_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
